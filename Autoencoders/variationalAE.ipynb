{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set working directory, so we don't download the same dataset twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = os.getcwd()\n",
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(working_dir))\n",
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input img -> Hidden dim -> mean, std -> Paramertication Trick -> Decoder -> Output img\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim=200, z_dim=20) -> None:\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.img_2hid = nn.Linear(input_dim, h_dim)\n",
    "        self.hid_2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hid_2sigma = nn.Linear(h_dim, z_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.z_2hid = nn.Linear(z_dim, h_dim)\n",
    "        self.hid_2img = nn.Linear(h_dim, input_dim)\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        # q_phi(z|x)\n",
    "        h = self.relu(self.img_2hid(x))\n",
    "        mu = self.hid_2mu(h)\n",
    "        sigma = self.hid_2sigma(h)\n",
    "        return mu, sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        # p_theta(x|z)\n",
    "        h = self.relu(self.z_2hid(z))\n",
    "        img = self.sigmoid(self.hid_2img(h))\n",
    "        return img\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = self.encode(x)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z_reparametrized = mu + sigma*epsilon\n",
    "        x_reconstructed = self.decode(z_reparametrized)\n",
    "        return x_reconstructed, mu, sigma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 784])\n",
      "torch.Size([4, 20])\n",
      "torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 28*28)\n",
    "vae = VariationalAutoEncoder(input_dim=784)\n",
    "x_reconstructed, mu, sigma = vae(x)\n",
    "print(x_reconstructed.shape)\n",
    "print(mu.shape)\n",
    "print(sigma.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current working device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Current working device: {device}\")\n",
    "INPUT_DIM = 784\n",
    "H_DIM = 200\n",
    "Z_DIM = 20\n",
    "NUM_EPOCH = 10\n",
    "BATCH_SIZE = 128\n",
    "LR_RATE = 3e-4 # Karpathy constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loading\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model params\n",
    "model = VariationalAutoEncoder(INPUT_DIM, H_DIM, Z_DIM).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "loss_fn = nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1875it [00:14, 131.20it/s, loss=3.99e+3]\n",
      "1875it [00:14, 133.33it/s, loss=4.25e+3]\n",
      "1875it [00:14, 132.31it/s, loss=4.21e+3]\n",
      "1875it [00:13, 134.38it/s, loss=3.99e+3]\n",
      "1875it [00:13, 134.12it/s, loss=4.09e+3]\n",
      "1875it [00:14, 133.86it/s, loss=3.83e+3]\n",
      "1875it [00:14, 129.94it/s, loss=4.07e+3]\n",
      "1875it [00:14, 131.37it/s, loss=4.1e+3] \n",
      "1875it [00:14, 131.39it/s, loss=4.25e+3]\n",
      "1875it [00:15, 121.11it/s, loss=3.93e+3]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    loop = tqdm(enumerate(train_loader))\n",
    "    for i, (x, _) in loop:\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(x.shape[0], INPUT_DIM)\n",
    "        x_reconstructed, mu, sigma = model(x)\n",
    "\n",
    "        # Compute loss\n",
    "        reconstructed_loss = loss_fn(x_reconstructed, x)\n",
    "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "\n",
    "        # Backprop\n",
    "        loss = reconstructed_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "def inference(digit, num_examples=1, out_path:Path=Path(\"./Autoencoders/VAE_gen_examples\")):\n",
    "    \"\"\"\n",
    "    Generates (num_exmaples) of a particular digit.\n",
    "    Specifically we extract an exmaple of eaxh digit, \n",
    "    then after we have mu, sigma representation for \n",
    "    each digit we can sample from that.\n",
    "\n",
    "    After we sample we can run the decoder part of the VAE and generate examples.\n",
    "    \"\"\"\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    images = []\n",
    "    idx = 0\n",
    "    for x, y in dataset:\n",
    "        if y == idx:\n",
    "            images.append(x)\n",
    "            idx += 1\n",
    "        if idx == 10:\n",
    "            break\n",
    "\n",
    "    encoding_digit = []\n",
    "    for d in range(10):\n",
    "        with torch.no_grad():\n",
    "            mu, sigma = model.encode(images[d]. view(1, 784))\n",
    "        encoding_digit.append((mu, sigma))\n",
    "    \n",
    "    mu, sigma = encoding_digit[digit]\n",
    "    for example in range(num_examples):\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z = mu + sigma * epsilon\n",
    "        out = model.decode(z)\n",
    "        out = out.view(-1, 1, 28, 28)\n",
    "        save_image(out, out_path / f\"generated_{digit}_ex{example}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    inference(idx, num_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
